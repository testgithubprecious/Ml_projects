{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy8B2ntJT/yVHlFjI2ptrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/testgithubprecious/Ml_projects/blob/main/AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm6VtlMEtsFp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install if not already:\n",
        "# pip install torch torchvision pillow matplotlib\n",
        "\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ------------------------------\n",
        "# Device\n",
        "# ------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ------------------------------\n",
        "# Simple CNN (MNIST)\n",
        "# ------------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 5 * 5, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Data\n",
        "# ------------------------------\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataset = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)  # batch_size=1 for demo\n",
        "\n",
        "# ------------------------------\n",
        "# FGSM attack (safe implementation)\n",
        "# ------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def fgsm_attack(model, images, labels, epsilon):\n",
        "    \"\"\"\n",
        "    Fast Gradient Sign Method (single-step).\n",
        "    images: tensor [B,C,H,W], on same device as model\n",
        "    labels: long tensor [B]\n",
        "    returns: perturbed images tensor [B,C,H,W]\n",
        "    \"\"\"\n",
        "    # Work on a detached copy so we don't clobber upstream tensors\n",
        "    images_adv = images.clone().detach().to(device)\n",
        "    images_adv.requires_grad_(True)\n",
        "\n",
        "    model.zero_grad()\n",
        "    outputs = model(images_adv)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    data_grad = images_adv.grad\n",
        "    perturbed = images_adv + epsilon * data_grad.sign()\n",
        "    perturbed = torch.clamp(perturbed, 0.0, 1.0)\n",
        "\n",
        "    # detach gradients to avoid keeping computation graph\n",
        "    return perturbed.detach()\n",
        "\n",
        "# ------------------------------\n",
        "# JPEG compression preprocessing (to mitigate small L_inf perturbations)\n",
        "# ------------------------------\n",
        "def jpeg_compression(image_tensor, quality=75):\n",
        "    \"\"\"\n",
        "    image_tensor: [B, C, H, W] float in [0,1], C=1 or 3\n",
        "    Returns tensor same shape, compressed via JPEG (PIL).\n",
        "    Works per-example (batch size 1 recommended for demo).\n",
        "    \"\"\"\n",
        "    # handle single image / batch\n",
        "    single = False\n",
        "    if image_tensor.dim() == 4 and image_tensor.size(0) == 1:\n",
        "        img = image_tensor.squeeze(0)\n",
        "        single = True\n",
        "    else:\n",
        "        # For simplicity, operate on first image if batch>1\n",
        "        img = image_tensor[0]\n",
        "\n",
        "    # Convert to PIL\n",
        "    if img.size(0) == 1:\n",
        "        pil = TF.to_pil_image(img.squeeze(0))  # grayscale\n",
        "    else:\n",
        "        pil = TF.to_pil_image(img)  # RGB\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    pil.save(buf, format=\"JPEG\", quality=int(quality))\n",
        "    buf.seek(0)\n",
        "    compressed = Image.open(buf).convert(pil.mode)\n",
        "    compressed_tensor = TF.to_tensor(compressed).unsqueeze(0)  # [1,C,H,W]\n",
        "\n",
        "    if single:\n",
        "        return compressed_tensor.to(image_tensor.device)\n",
        "    else:\n",
        "        # replace first example only (keeps shape)\n",
        "        out = image_tensor.clone()\n",
        "        out[0] = compressed_tensor.to(image_tensor.device)\n",
        "        return out\n",
        "\n",
        "# ------------------------------\n",
        "# Adversarial training\n",
        "# ------------------------------\n",
        "def train_with_adversarial(model, loader, epsilon=0.2, epochs=2):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Generate adversarial examples ON THE FLY using the current model\n",
        "            adv_images = fgsm_attack(model, images.clone(), labels, epsilon)\n",
        "\n",
        "            # Combine clean + adversarial (double the batch)\n",
        "            images_combined = torch.cat([images, adv_images], dim=0)\n",
        "            labels_combined = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "            outputs = model(images_combined)\n",
        "            loss = criterion(outputs, labels_combined)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"üìò Epoch {epoch+1}/{epochs} - adversarial training done\")\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluation under attack (with optional preprocessing)\n",
        "# ------------------------------\n",
        "def evaluate_under_attack(model, loader, epsilon=0.2, use_preprocessing=False, max_samples=100):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Generate adversarial examples (requires gradients, so do without torch.no_grad)\n",
        "            adv_images = fgsm_attack(model, images.clone(), labels, epsilon)\n",
        "\n",
        "            if use_preprocessing:\n",
        "                adv_images = jpeg_compression(adv_images.cpu()).to(device)\n",
        "\n",
        "            outputs = model(adv_images)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            if total >= max_samples:\n",
        "                break\n",
        "    return correct / total if total > 0 else 0.0\n",
        "\n",
        "# ------------------------------\n",
        "# Demo: train and evaluate\n",
        "# ------------------------------\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# Adversarial train for a couple epochs (demo)\n",
        "train_with_adversarial(model, train_loader, epsilon=0.2, epochs=2)\n",
        "\n",
        "# Evaluate on 100 samples for speed (no defense vs JPEG defense)\n",
        "acc_no_defense = evaluate_under_attack(model, test_loader, epsilon=0.2, use_preprocessing=False, max_samples=100)\n",
        "acc_with_jpeg = evaluate_under_attack(model, test_loader, epsilon=0.2, use_preprocessing=True, max_samples=100)\n",
        "\n",
        "print(f\"‚ö†Ô∏è Accuracy against FGSM (no defense): {acc_no_defense:.2%}\")\n",
        "print(f\"üõ°Ô∏è Accuracy with JPEG preprocessing: {acc_with_jpeg:.2%}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Visualize one example (original, adversarial, compressed)\n",
        "# ------------------------------\n",
        "model.eval()\n",
        "data_iter = iter(test_loader)\n",
        "images, labels = next(data_iter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "adv_images = fgsm_attack(model, images.clone(), labels, epsilon=0.2)\n",
        "adv_images_jpeg = jpeg_compression(adv_images.cpu()).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    orig_pred = model(images).argmax(dim=1).item()\n",
        "    adv_pred = model(adv_images).argmax(dim=1).item()\n",
        "    adv_jpeg_pred = model(adv_images_jpeg).argmax(dim=1).item()\n",
        "\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(f\"Original\\npred={orig_pred}\")\n",
        "plt.imshow(images.squeeze().cpu().numpy(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(f\"Adversarial\\npred={adv_pred}\")\n",
        "plt.imshow(adv_images.squeeze().cpu().numpy(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(f\"Adversarial + JPEG\\npred={adv_jpeg_pred}\")\n",
        "plt.imshow(adv_images_jpeg.squeeze().cpu().numpy(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ]
}