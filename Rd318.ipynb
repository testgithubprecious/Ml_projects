{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjvcAN7C/bzQMCi7fPLuNl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/testgithubprecious/Ml_projects/blob/main/Rd318.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm6VtlMEtsFp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install dependencies:\n",
        "# pip install torch torchvision opencv-python\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------\n",
        "# Load pretrained action recognition model\n",
        "# ------------------------------\n",
        "model = torchvision.models.video.r3d_18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Partial Kinetics-400 class labels (for demo)\n",
        "kinetics_classes = [\n",
        "    \"abseiling\", \"air drumming\", \"answering questions\", \"applauding\", \"applying cream\",\n",
        "    \"archery\", \"arm wrestling\", \"arranging flowers\", \"assembling computer\", \"auctioning\"\n",
        "]\n",
        "\n",
        "# ------------------------------\n",
        "# Video loader and preprocessor\n",
        "# ------------------------------\n",
        "def load_video_frames(path, num_frames=16, size=(112, 112)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    step = max(total // num_frames, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, size)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    # Transform and normalize\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
        "                    std=[0.22803, 0.22145, 0.216989])\n",
        "    ])\n",
        "    frames = [transform(frame) for frame in frames]\n",
        "    video = torch.stack(frames).permute(1, 0, 2, 3)  # [C, T, H, W]\n",
        "    return video.unsqueeze(0)  # [1, C, T, H, W]\n",
        "\n",
        "# ------------------------------\n",
        "# Path to video and preprocessing\n",
        "# ------------------------------\n",
        "video_path = \"sample_action.mp4\"  # Replace with your video path\n",
        "video_tensor = load_video_frames(video_path)\n",
        "\n",
        "# ------------------------------\n",
        "# Predict action\n",
        "# ------------------------------\n",
        "with torch.no_grad():\n",
        "    outputs = model(video_tensor)\n",
        "    probs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "    top5 = torch.topk(probs, k=5)\n",
        "\n",
        "# ------------------------------\n",
        "# Print top predictions\n",
        "# ------------------------------\n",
        "print(\"ðŸŽ¬ Top 5 Predicted Actions:\")\n",
        "for idx in top5.indices:\n",
        "    print(f\"{kinetics_classes[idx]} ({probs[idx]*100:.2f}%)\")"
      ]
    }
  ]
}